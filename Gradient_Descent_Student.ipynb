{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKWq5CRbZQ0tCHaxwbQSL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ISEA-Repositories/MLsessions/blob/main/Gradient_Descent_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A step-by-step walkthrough of gradient descent with OLS\n",
        "\n",
        "In this colab, you will learn to use gradient descent to calculate the slope and intercept of a line.\n"
      ],
      "metadata": {
        "id": "iWbI71qfpgCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will set up some data and plot it. We will also calculate the OLS coefficients using a pre-built function"
      ],
      "metadata": {
        "id": "SV2A2qbmp0Kf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFRQXBDeQB-l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "n_observations = 100\n",
        "\n",
        "x = np.random.rand(n_observations) * 10  # Values between 0 and 10\n",
        "\n",
        "# Define the true relationship (linear with some noise)\n",
        "true_slope = 2\n",
        "true_intercept = 1\n",
        "noise_level = 1\n",
        "\n",
        "# Generate y values based on the true relationship and add some noise\n",
        "y = true_slope * x + true_intercept + np.random.normal(0, noise_level, n_observations)\n",
        "\n",
        "# Estimate model using OLS\n",
        "X_ols = sm.add_constant(x)  # Add intercept term\n",
        "model = sm.OLS(y, X_ols).fit()\n",
        "estimated_intercept, estimated_slope = model.params\n",
        "y_estimated = estimated_slope * x + estimated_intercept\n",
        "\n",
        "# Compute Mean Squared Error (MSE)\n",
        "y_pred = estimated_slope * x + estimated_intercept\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "\n",
        "\n",
        "# Create a plot\n",
        "plt.scatter(x, y, color='blue', marker='o', edgecolors='black')\n",
        "plt.plot(x, y_estimated, color='green', linestyle='--', linewidth=2, label=\"OLS\")\n",
        "\n",
        "plt.xlabel(\"X-axis\")\n",
        "plt.ylabel(\"Y-axis\")\n",
        "plt.title(\"Sample Scatter Plot\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "print(f\"True Intercept: {true_intercept}, True Slope: {true_slope}\")\n",
        "print(f\"Estimated Intercept: {estimated_intercept:.4f}, Estimated Slope: {estimated_slope:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Questions:\n",
        "- Why isn't the estimated intercept 1?\n",
        "- Why isn't the estimated slope 2?\n",
        "- The MSE is 0.8, do we expect MSE to ever be 0 (with real data)?"
      ],
      "metadata": {
        "id": "HyqGxXalqABz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Univariate OLS, the gradient way\n",
        "\n",
        "In this implementation, we will **not** use linear algebra or a pre-canned function to solve OLS. Instead we will use gradient descent!\n",
        "\n",
        "\n",
        "A simple linear model is:  \n",
        "\n",
        "$$\n",
        "y = m \\mathbf{X} + b\n",
        "$$\n",
        "\n",
        "We want to identify $m, b$ given $X, y$\n",
        "\n",
        "1. Define a cost function - for OLS we use Mean Squared Error (MSE) $$\n",
        "C(m, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\hat{m} x_i + \\hat{b}))^2\n",
        "$$\n",
        "1. Set initial, arbitrary values for $m, b$ (lets use 0,0)\n",
        "1. Predict $y$. $$\n",
        "\\hat{y} = m x + b\n",
        "$$\n",
        "1. Compute the gradient: find the partial derivative of the cost function for m and b.\n",
        "  - $$\n",
        "\\frac{\\partial C}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) $$\n",
        "  - $$\n",
        "\\frac{\\partial C}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "1. Update $m, b$ using the gradient and a learning rate\n",
        "1. Repeat 3 to 5\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zIHCu1nTSjBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Work\n",
        "- Note: Use the top menu `Runtime > Restart Session` to reset your cells if things start getting weird\n",
        "- I reccomend having one person share their screen as you work through this"
      ],
      "metadata": {
        "id": "dEs8iLMWviG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Questions\n",
        "- Explain the cost function in your own words $$ C(m, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\hat{m} x_i + \\hat{b}))^2\n",
        "$$\n",
        "- Explain what a partial derivative is"
      ],
      "metadata": {
        "id": "-sg79iq-uTuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the cost function (Mean Squared Error)\n",
        "def compute_cost(m, b, x, y):\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "6FrGSg2pSiW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2, 3: Set initial 0 values for m and b, and predict y\n",
        "m = 0  # Initial slope\n",
        "b = 0  # Initial intercept\n",
        "\n",
        "def predict(m, b, x):\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "y_pred = predict(m, b, x)\n",
        "\n",
        "print(y[0:4]) # print the first 5 values as a check\n",
        "print(y_pred[0:4])\n",
        "print(compute_cost(m, b, x, y))"
      ],
      "metadata": {
        "id": "2TM2aoYEYrFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick check, you should be printing all 0's for y_pred above. Why?"
      ],
      "metadata": {
        "id": "cVUpALLMv0T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4, 5: Compute the gradient, update m and b\n",
        "def compute_gradients(x, y, y_pred):\n",
        "    # your code here, reference the equations provided above\n",
        "    # You can use the function `np.sum` for the summation notation\n",
        "\n",
        "    return dm, db #  Gradient with respect to m, b\n",
        "\n",
        "def update_parameters(m, b, dm, db, learning_rate=0.01):\n",
        "    # your code here\n",
        "    return m, b\n",
        "\n",
        "dm, db = compute_gradients(x, y, y_pred)\n",
        "print(f\"m-gradient = {dm:.1f}, b-gradient = {db:.1f}\")\n",
        "\n",
        "m, b = update_parameters(m, b, dm, db)\n",
        "print(f\"New m = {m:.2f}, New b = {b:.2f}\")\n",
        "print(f\"Cost = {compute_cost(m, b, x, y)}\")"
      ],
      "metadata": {
        "id": "orxcOzJ8ZDca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the output above, what would happen if we didn't use a learning rate? Did cost go up or down?"
      ],
      "metadata": {
        "id": "MgS1p9cAwXQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put it together\n",
        "Create a loop to repeat steps 3-5, run it three times print m, b and cost for each step"
      ],
      "metadata": {
        "id": "n4Sb0MbckjfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "HLTSxLlBa11r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now run it 1000 times. Only print final versions of m, b, and cost\n",
        "\n",
        "\n",
        "print(f\"m = {m:.2f}, b = {b:.2f}, cost = {compute_cost(m, b, x, y):.2f}\")"
      ],
      "metadata": {
        "id": "6j5k7pzwb8DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are some possible stopping rules for this loop?\n",
        "Designate one person from your group to talk through your solution"
      ],
      "metadata": {
        "id": "wWdopUOAojcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finished early?\n",
        "- Discuss with your group applied education ML cases you are familiar with"
      ],
      "metadata": {
        "id": "MM68ft_rmqrt"
      }
    }
  ]
}