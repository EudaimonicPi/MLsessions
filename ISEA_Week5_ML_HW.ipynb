{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EudaimonicPi/MLsessions/blob/main/ISEA_Week5_ML_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Task: Predict Student Success\n",
        "\n",
        "The purpose of this HW is to get you hands on with real data trying out the modelling techniques we talked about.\n",
        "\n",
        "You are free to use gen-ai with this project to help with the coding (of course, you don't have to!). [Hands on Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) is also a great resource.\n",
        "\n",
        "Your code needs to run, but I want you to focus less on the specifics of the code and more on understanding the component steps that go into building and validating a model. Creating code is now pretty easy, creating a \"good\" model is hard.\n",
        "\n",
        "For this exercise we will use open data on student dropout from Portugal. Full documentation is available [here](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)\n",
        "\n",
        "M.V.Martins, D. Tolledo, J. Machado, L. M.T. Baptista, V.Realinho. (2021) \"Early prediction of studentâ€™s performance in higher education: a case study\" Trends and Applications in Information Systems and Technologies, vol.1, in Advances in Intelligent Systems and Computing series. Springer. DOI: 10.1007/978-3-030-72657-7_16\n",
        "\n",
        "You will turn in on the class website a google slide deck that has:\n",
        "1. A cover slide contianing your name (and all group member names if working together) and a link to your colab (**Create slide 1 now**)\n",
        "2. 3 slides answering the questions below - they are clearly indicated as you go through the colab notebook.\n"
      ],
      "metadata": {
        "id": "0MhB7souuE8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the data"
      ],
      "metadata": {
        "id": "q5hWeXbc2eK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I provide some code to get the data for you"
      ],
      "metadata": {
        "id": "bJd4Kju6LX7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo"
      ],
      "metadata": {
        "id": "TL-Z4gGctdTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch dataset\n",
        "predict_students_dropout_and_academic_success = fetch_ucirepo(id=697)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = predict_students_dropout_and_academic_success.data.features\n",
        "y = predict_students_dropout_and_academic_success.data.targets\n",
        "\n",
        "# metadata\n",
        "print(predict_students_dropout_and_academic_success.metadata)\n",
        "\n",
        "# variable information\n",
        "print(predict_students_dropout_and_academic_success.variables)"
      ],
      "metadata": {
        "id": "EYQXaH6itafW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Data Checking\n",
        "\n",
        "- Look at your outcome variable - any cases to exclude?\n",
        "- Determine the base-rate accuracy for a naive model\n",
        "- Create Test and Training Sets\n",
        "- Look at distributions of x variables, look up meta data, decide which to include\n",
        "\n",
        "At the end of this section you should have\n",
        "`x_train`, `x_text`, `y_train`, `y_test`\n",
        "And an estimate of the base rate accuracy."
      ],
      "metadata": {
        "id": "DwkrBnDEwtI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" the goal of this function is to get an estimate of the base rate.\n",
        "    the base rate is the proportion of the most frequent outcome\n",
        "\"\"\"\n",
        "def get_counts():\n",
        "    # helper function that gets graduate and dropout counts\n",
        "    grad_count = 0\n",
        "    dropout_count = 0\n",
        "\n",
        "    # y is a df, so we have to iterate through the \"Target\" column\n",
        "    for target in y['Target']:\n",
        "        if target == \"Graduate\":\n",
        "            grad_count += 1\n",
        "        elif target == \"Dropout\":\n",
        "            dropout_count += 1\n",
        "        elif target == \"Enrolled\":\n",
        "            continue\n",
        "        else:\n",
        "            print(\"We have a missing target \", target)\n",
        "    return grad_count, dropout_count\n",
        "\n",
        "def estimate_base_rate():\n",
        "    # what is the most frequent outcome (amongst base & target?)\n",
        "    # let's calculate \"Graduate\" and \"Dropout\" rates to find out (and exclude \"Enrolled\" outcomes)\n",
        "    grad_count, dropout_count = get_counts()\n",
        "\n",
        "    print(\"Graduate count is \" ,grad_count) # 2209\n",
        "    print(\"Droupout count is \", dropout_count) # 1421\n",
        "    total_count = grad_count + dropout_count\n",
        "    # if grad_count >= dropout_count: # hmmm... what IF they were equal?\n",
        "    #     return grad_count/total_count\n",
        "    # else:\n",
        "    return dropout_count/total_count\n",
        "\n",
        "# WE HAVE TO FILTER THE DATASET FROM THOSE THAT ARE CURRENTLY ENROLLED\n",
        "# does that require you to get the X values corresponding to the y values that are either grad or dropout?\n",
        "\n",
        "# can split 70/30\n",
        "# how to get length?\n",
        "# x_train = X[:5]\n",
        "# x_test =\n",
        "# print(len(X.columns))\n",
        "# print(\"Amount of X vector is \", len(X))\n",
        "# print(\"Amount of y is \", len(y))\n",
        "# x_test = X\n",
        "\n",
        "def split_dataset():\n",
        "    grad_count, dropout_count = get_counts()\n",
        "    total = grad_count + dropout_count\n",
        "    # could join the data based on rows\n",
        "    # then if the 'Target' column is 'Enrolled', filter it out\n",
        "    joint_xy_df = X.join(y)\n",
        "    filtered_df = joint_xy_df[joint_xy_df['Target'] != \"Enrolled\"]\n",
        "    # print(filtered_df.head(4))\n",
        "\n",
        "    # calculate length of new data set\n",
        "    dataset_length = len(filtered_df)\n",
        "    # print(\"Filtered df length is \", dataset_length)2\n",
        "    # get .7, get .3 number\n",
        "    training_percentage = .7\n",
        "    testing_percentage = 1 - training_percentage\n",
        "\n",
        "    num_train = int(training_percentage*dataset_length) # this num matters for indexing purposes\n",
        "    num_test = int(testing_percentage*dataset_length)\n",
        "\n",
        "    # with those numbers, split the data set\n",
        "    print(\"Number of training and number of testing\", num_train, num_test)\n",
        "\n",
        "    # now we've got to split the data set to get separate x_train and y_train, right?\n",
        "    filtered_y_df = filtered_df[['Target']]\n",
        "    filtered_x_df = filtered_df.drop('Target', axis = 1)\n",
        "\n",
        "    x_train = filtered_x_df[:num_train]\n",
        "    x_test = filtered_x_df[num_train:]\n",
        "    y_train = filtered_y_df[:num_train]\n",
        "    y_test = filtered_y_df[num_train:]\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "base_accuracy = estimate_base_rate()\n",
        "print(\"Base accuracy estimate is , \", base_accuracy)\n",
        "x_train, x_test, y_train, y_test = split_dataset()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "NOTES\n",
        " * The outcome variable is y, meaning the actual outcomes (not y_pred)\n",
        " * base-rate accuracy is the proportion of the most frequent\n",
        " *\n",
        " *\n",
        " *\n",
        " *\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "QUESTIONS\n",
        " * Should the total for base rate accuracy exclude \"Enrolled\"\n",
        " *\n",
        " *\n",
        " *\n",
        " *\n",
        "\"\"\"\n",
        "# what is the outcome variable, y_pred. No it's y (actual)\n",
        "\n",
        "# what is base-rate accuracy? is that just (TP + TN/ (TP + TN + FP + FN)) # how to determine that?\n",
        "# base-rate accuracy is the proportion of the most frequent\n",
        "# count dropouts vs graduates and deyermine most frequent\n",
        "\n",
        "# create test and training sets, from the raw data?\n",
        "# looking at the distribtuions through what kind of measures?\n",
        "# could this just be done with an 80 20 split => chat suggested 70/80 ti 30/20"
      ],
      "metadata": {
        "id": "FB5j4I-xwsEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Train a Model\n",
        "* Pick one of the models we discussed today and train it.\n",
        "* Report its accuracy and print a confusion matrix.\n",
        "   * How much better is your model than the base rate?\n",
        "   * How does accuracy on the train set compare to accuracy on the test set?\n",
        "   * **Report Slide 2: Include an image of the confusion matrix, the base rate accuracy, train-set accuracy and test-set accuracy**"
      ],
      "metadata": {
        "id": "x0Rxs5oOxEhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# goal: our model should return a vector of y_pred\n",
        "# how to train the weights, these are adjusted through gradient descent\n",
        "# x_train, y_train\n",
        "\n",
        "\n",
        "\n",
        "#tbh probably an automatic way to do this :)\n",
        "# def compute_cost(m, b, x, y):\n",
        "#     n = len(x)\n",
        "#     y_pred = sigmoid(np.dot(x, m) + b)  # Apply sigmoid for binary classification\n",
        "#     cost = (-1/n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))  # Binary Cross-Entropy Loss\n",
        "#     return cost\n",
        "\n",
        "def compute_cost(m, b, x, y):\n",
        "    n = len(x)\n",
        "\n",
        "    # Ensure y and y_pred are NumPy arrays\n",
        "    y = np.array(y)  # Convert y to a NumPy array if it's a list\n",
        "    y_pred = sigmoid(np.dot(x, m) + b)  # Apply sigmoid for predictions\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "\n",
        "    # Compute the binary cross-entropy cost\n",
        "    cost = (-1/n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "    return cost\n",
        "# def compute_cost(m, b, x, y):\n",
        "#     # Number of data points\n",
        "#     n = len(x)\n",
        "#     # Compute the predictions\n",
        "#     y_pred = np.dot(x, m) + b  # Assuming x is a 2D matrix, and m is a vector (dot product)\n",
        "#     # Calculate the cost using the Mean Squared Error (MSE) formula\n",
        "#     cost = (1 / (2 * n)) * np.sum((y_pred - y) ** 2)\n",
        "#     return cost\n",
        "\n",
        "def sigmoid(z):\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def predict(m, b, x): # goes from linear to sigmoid\n",
        "    z = np.dot(x, m) + b  # Linear combination of inputs and weights\n",
        "    return sigmoid(z)  # Apply sigmoid to get probabilities\n",
        "\n",
        "# confusion matrix (TP, TN, FP, FN)\n",
        "# base rate accuracy: proportion of most frequent target\n",
        "# train-set accuracy: TP + TN/all for train\n",
        "# test-set: TP + TN/all for test\n",
        "# def predict(m, b, x):\n",
        "#     # y = mx*b\n",
        "#     # y pred is a vector with mx + b applied to each par\n",
        "#     y_pred = m*x + b # this is vector multiplication (m is vec, no?)\n",
        "#     return y_pred\n",
        "\n",
        "def compute_gradients(x, y, y_pred):\n",
        "    # your code here, reference the equations provided above\n",
        "    # You can use the function `np.sum` for the summation notation\n",
        "    n = len(x)\n",
        "    dm = (-1/n) * np.dot(x.T, (y - y_pred))  # Compute gradient for m (weights)\n",
        "    db = (-1/n) * np.sum(y - y_pred)  # Compute gradient for b (bias)\n",
        "\n",
        "    return dm, db #  Gradient with respect to m, b\n",
        "\n",
        "def update_parameters(m, b, dm, db, learning_rate=0.01):\n",
        "    # your code here\n",
        "    # print(\"LENGTH OF DM AND DB\", len(dm), len(db))\n",
        "    m = m - learning_rate * dm # neg grad descent here?\n",
        "    b = b - learning_rate *db\n",
        "    return m, b\n",
        "\n",
        "def initialize_weights_xavier(n_features):\n",
        "    # Xavier initialization (good for deep networks)\n",
        "    limit = np.sqrt(6) / np.sqrt(n_features + 1)\n",
        "    m = np.random.uniform(-limit, limit, (n_features, 1))\n",
        "\n",
        "    b = 0  # Bias is usually initialized to 0\n",
        "    return m\n",
        "\n",
        "\n",
        "def train_ML(m, b, x, y, epochs = 10):\n",
        "    # Replacing values in the 'Target' column\n",
        "    y['Target'] = y['Target'].replace({'Graduate': 0, 'Dropout': 1})\n",
        "\n",
        "    # Convert the 'Target' column to a list\n",
        "    y = y['Target'].tolist()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # make predictions\n",
        "      y_pred = predict(m,b,x)\n",
        "\n",
        "      # compute gradients\n",
        "      dm, db = compute_gradients(x, y, y_pred) # HERE dm and db hmm...\n",
        "      # print(\"dm and db are \", dm, db)\n",
        "\n",
        "      # print(f\"m-gradient = {dm:.1f}, b-gradient = { db:.1f}\")\n",
        "\n",
        "      # update parameters\n",
        "      m, b = update_parameters(m, b, dm, db)\n",
        "      # print(\"m and b are now, \" ,m, b)\n",
        "    #   # print(f\"New m = {m:.2f}, New b = { b:.2f}\")\n",
        "\n",
        "      cost = compute_cost(m, b, x, y)\n",
        "      print(f\"Epoc {epoch}:M Cost = {cost:.4f}\")\n",
        "\n",
        "    # print(\"Final y is \", y_pred)\n",
        "dataset_length = len(x_train)\n",
        "# m_vector = np.zeros((1, 36))\n",
        "initial_bias = 0\n",
        "# m_vector2 = np.zeros(( 36, 1))\n",
        "m_vector2 = initialize_weights_xavier(36)\n",
        "# print(\"HI\")\n",
        "train_ML(m_vector2, initial_bias, x_train, y_train)\n",
        "\n",
        "\n",
        "# row by col, num rows is 1 for each feature, num cols is 1 for each input\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r8H9DxK7xdGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Train a Different Model\n",
        "* Repeat all the steps in 2, but use a different model\n",
        "* In addition, compare the accuracy of 1 and 2\n",
        "* **Report Slide 3: Model 2 confusion matrix, train-set accuracy and test-set accuracy. Comparison Model 1 and Model 2 accuracy**"
      ],
      "metadata": {
        "id": "yBAw-iRRxeAT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RFrV7VOx33n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Reflection\n",
        "* **Type responses on Slide 4**\n",
        "* Contextualizing accuracy - think about different use cases for your model, which ones would you feel its accurate enough to use for? I only asked you to look at overall accuracy, is that good enough?\n",
        "* Contextualizing features - think about these same use cases, are the prediction features you included appropriate for these uses?\n",
        "* Generalizability - again thinking about your features, could you use this model in other educational contexts? How hard would it be to get that same data? Are there issues with it generalizing over time and location?"
      ],
      "metadata": {
        "id": "lHlWSfEIx9na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Extra Credit\n",
        "* Consider ensembling your two models. Does that perform better?\n",
        "* Check accuracy for different subgroups"
      ],
      "metadata": {
        "id": "th7YsFJJ1ilE"
      }
    }
  ]
}